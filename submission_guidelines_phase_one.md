Submission Guidelines (Phase 1 - Green Agent)
Submission Requirements
Abstract: Brief description of the tasks your green agent evaluates
Public GitHub repository: Complete source code and README describing how to run the green agent
Baseline purple agent(s): A2A-compatible purple/competition agent(s) showing how the benchmark is evaluated
Docker image: Packaged green agent that runs end-to-end without manual intervention
AgentBeats registration: Register your green agent and baseline purple agent(s) on the AgentBeats developer platform (coming soon–stay tuned!)
Demo video: Up to 3 minutes demonstrating your green agent
Judging Criteria
Technical Correctness, Implementation Quality and Documentation
Clean, well-documented code with clear README with overview, setup, and usage instructions
Docker image builds and runs without issues
Reasonable resource requirements (compute, memory, time)
Robust error handling and logging
Correct task logic and scoring
Reproducibility: Is the benchmark consistent and easy for any A2A‑compatible agent to run?
Consistent results across runs with the same agents
Benchmark Design Quality: How meaningful and well‑designed are the tasks and evaluation?
Tasks are realistic, meaningful, and representative of real-world agent capabilities
Clear difficulty progression or diverse skill assessment
Tasks genuinely test agentic capabilities (e.g., reasoning, planning, multi-step execution) or safety/security issues
Avoids trivial tasks or those easily solved by simple heuristics
Evaluation Methodology
Clear, objective, and justifiable scoring criteria
Automated evaluation where possible
Appropriate metrics for the task type
Goes beyond binary pass/fail to provide nuanced evaluation
Captures multiple dimensions of agent performance (accuracy, efficiency, safety, etc.)
Innovation & Impact
Original contribution to the evaluation landscape
For agentifying existing benchmark: extensions beyond simple agentification
For new benchmarks: addresses gaps in existing evaluation coverage
Creative approach to difficult-to-evaluate capabilities
Clear use case and target audience
Complementary to (not redundant with) existing benchmarks